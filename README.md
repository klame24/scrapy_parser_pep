# Асинхронный парсер PEP документов

Асинхронный парсер документов PEP (Python Enhancement Proposals) на основе фреймворка Scrapy.

## Описание проекта

Парсер собирает информацию о всех PEP с официального сайта Python и сохраняет её в двух CSV-файлах:

1. **pep_ДатаВремя.csv** - полный список всех PEP с их номерами, названиями и статусами
2. **status_summary_ДатаВремя.csv** - сводка по количеству PEP в каждом статусе

## Возможности

- Асинхронный парсинг для высокой производительности
- Сбор полной информации о каждом PEP
- Автоматическое сохранение результатов в формате CSV
- Формирование статистики по статусам PEP
- Логирование процесса работы

## Требования

- Python 3.9+
- Scrapy 2.5+
- Дополнительные зависимости указаны в `requirements.txt`

## Установка

1. Клонируйте репозиторий:
```bash
git clone https://github.com/klame24/scrapy_parser_pep
cd scrapy_parser_pep
```

2. Создайте виртуальное окружение:
```bash
python -m venv venv
```

3. Активируйте виртуальное окружение:
- Для Linux/Mac:
```bash
source venv/bin/activate
```
- Для Windows:
```bash
venv\Scripts\activate
```

4. Установите зависимости:
```bash
pip install -r requirements.txt
```

## Использование

### Запуск парсера

```bash
scrapy crawl pep
```

### Просмотр результатов

После выполнения парсинга в директории `results/` появятся два файла:
- `pep_YYYY-MM-DD_HH-MM-SS.csv` - список всех PEP
- `status_summary_YYYY-MM-DD_HH-MM-SS.csv` - сводка по статусам


## Компоненты парсера

### 1. Паук (Spider) - `pep_parse/spiders/pep.py`
- Парсит главную страницу с индексом PEP
- Собирает ссылки на все документы PEP
- Обрабатывает каждую страницу PEP для извлечения данных

### 2. Модели данных (Items) - `pep_parse/items.py`
```python
class PepParseItem(scrapy.Item):
    number = scrapy.Field()   # Номер PEP
    name = scrapy.Field()     # Название PEP
    status = scrapy.Field()   # Статус документа
```

### 3. Конвейер (Pipeline) - `pep_parse/pipelines.py`
- Собирает статистику по статусам PEP
- Создает сводный файл с количеством документов по статусам
- Автоматически создает директорию для результатов

### 4. Настройки - `pep_parse/settings.py`
- Конфигурация Scrapy
- Настройки экспорта данных в CSV
- Конфигурация пайплайнов

## Тестирование

Для запуска тестов выполните:

```bash
pytest
```

Тесты проверяют:
- Корректность структуры проекта
- Работу паука
- Корректность пайплайнов
- Создание выходных файлов
- Формат данных

## Особенности реализации

### Парсинг страниц
- Используются CSS-селекторы и XPath для навигации по DOM
- Обработка различных форматов заголовков PEP
- Извлечение статуса документа из информационной таблицы

### Обработка данных
- Автоматическое создание директории для результатов
- Форматирование имен файлов с временными метками
- Корректная кодировка UTF-8 для сохранения данных

### Обработка ошибок
- Проверка наличия данных перед обработкой
- Значения по умолчанию для отсутствующих полей
- Логирование процесса для отладки

## Пример работы

```bash
# Запуск парсера
$ scrapy crawl pep

# Выходные данные
2026-01-12 14:44:16 [scrapy.utils.log] INFO: Scrapy 2.5.1 started
2026-01-12 14:44:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://peps.python.org/>
...
2026-01-12 14:44:17 [scrapy.core.engine] INFO: Closing spider (finished)

# Проверка результатов
$ ls results/
pep_2026-01-12_14-44-17.csv
status_summary_2026-01-12_14-44-17.csv
```
